---
layout: about
title: about
permalink: /
subtitle: >
  Hi, I am Bálint. Thanks for checking on my home page! <br/>
  <em>(My name is pronounced BAH-lint [baːlint])</em>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: true # crops the image to make it circular
  more_info: >
    PhD candidate at the <a href="https://agents.inf.ed.ac.uk/">Autonomous Agents Research Group</a> in explainable AI at the University of Edinburgh.
news: true # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---


I research trustworthy _explainable autonomous agency_ in multi-agent systems for AI safety, with applications to autonomous vehicles.
I like to describe this as _giving AI agents the ability to explain themselves._

I am lucky to be supervised by three amazing scientists: [Stefano Albrecht](https://agents.inf.ed.ac.uk/stefano-albrecht/), [Shay Cohen](https://homepages.inf.ed.ac.uk/scohen/), [Chris Lucas](https://homepages.inf.ed.ac.uk/clucas2/).

Early on in my PhD, I realised quickly that the current state of _explainable AI (XAI) is unsustainable._
Existing methods only work for some models, with particular asumptions, they ignore OOD data, always assume that the AI agent is correct, and so on...

The biggest issue however is that _explanations are simply not made to help users_ who interact with AI agents.

I am interested in exploring better ways to create intelligible explanations to _calibrate trust according to the abilities of the AI agents_.

I also research the epistemic backgrounds of AI ethics and AI safety to understand how explanations can help create more transparent systems considering both the short- and long-term risks of AI.

<br />

<img src="assets/img/background.jpg" alt="View of a corrie in the Scottish Highlands" width="100%" />
_View of a corrie from Beinn Bhan near Applecross, Scotland shot by me._

<br />