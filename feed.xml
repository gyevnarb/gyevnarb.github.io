<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://gyevnarb.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gyevnarb.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-24T14:55:09+00:00</updated><id>https://gyevnarb.github.io/feed.xml</id><title type="html">Bálint Gyevnár</title><subtitle>Postdoctoral research associate at Carnegie Mellon University researching the metascience of artificial intelligence. Previously, researching explainable multi-agent reinforcement learning. </subtitle><entry><title type="html">Mathematical Understanding and Artificial Intelligence</title><link href="https://gyevnarb.github.io/blog/2025/math-understand-cogsci/" rel="alternate" type="text/html" title="Mathematical Understanding and Artificial Intelligence"/><published>2025-11-10T15:40:16+00:00</published><updated>2025-11-10T15:40:16+00:00</updated><id>https://gyevnarb.github.io/blog/2025/math-understand-cogsci</id><content type="html" xml:base="https://gyevnarb.github.io/blog/2025/math-understand-cogsci/"><![CDATA[ <blockquote class="epigraph"> <div class="epigraph-title">Sneaking around ∞</div> <p> <i>Dedekind’s idea of infinite sets:</i> <br/> if you’re one of them you’re simply a set<br/> admitting 1-to-1 correspondence <br/> with—wait:— a <i>proper</i> subset <br/> of yourself!. . . you’re a snake <br/> that can gobble a snake your size <br/> with room left over besides. . . <br/> not worrying that in your meal <br/> yet another same size snake resides; <br/> stream upon stream <br/> of <i>mise en abîme.</i> </p> <footer>— Barry Mazur; November 5, 2025<d-footnote>As we shall see, thinking about mathematical understanding is much like thinking about infinite sets. A question that requires thinking about questions which require thinking about questions, and so on, until…, we find ourselves rethinking mathematical understanding again.</d-footnote></footer> </blockquote> <h3 id="knowledge">Knowledge</h3> <p>What does it mean to understand mathematics?</p> <p>Maybe mathematical understanding is having factual knowledge of mathematical concepts.</p> <p>Well, that could hardly be <i>it</i>. As <a href="https://www.vanderbilt.edu/psychological_sciences/bio/bethany-rittle-johnson">Bethany Rittle-Johnson</a> explained, very young children may know the number words from “one” to “five”, but when asked for five coins, they fail to understand the concept of size (i.e. cardinality of a set)<d-cite key="frye1989YoungChildrens"></d-cite>. In another example, students who had only seen the “equals sign” in expressions such as “5 + 3 = __” thought that they know what the “equals sign” represented, but then failed to answer questions such as “5 + 3 = 2 + __”, thinking that the “equals sign” stood for a sum<d-cite key="rittle-johnson1999ConceptualProcedural"></d-cite>.</p> <p>There is an interesting tension in these examples, and more broadly in maths education, so argued <a href="https://gse.rutgers.edu/faculty/keith-weber/">Keith Weber</a>, between reconciling the internal representations we have of concepts (e.g. the relationship between number words and cardinality) with the external, institutional representations imposed on us (e.g. the ways the “equals sign” <i>should</i> be interpreted)<d-cite key="dawkins2017ValuesNorms"></d-cite>. For maths education, metaphors are of great use here. For example, a proof is a hike through the forest and sets are containers. This may be why, asking mathematicians Akshay Venkatesh and Barry Mazur about the most important aspects of their work, they replied with “examples and analogies”.</p> <p>Such an internal-external representational conflict is one we also often see in artificial intelligence (AI). The reason a large language model (LLM) cannot answer the question <i>“How many Rs are there in the word strawberry?”</i><d-footnote>AIs on Rs in "strawberry": <a href="https://languagelog.ldc.upenn.edu/nll/?p=65667">https://languagelog.ldc.upenn.edu/nll/?p=65667</a></d-footnote> is (roughly) because LLMs operate on “token-language” not English. Similarly, if you ask an LLM to read a clock, it fails miserably<d-cite key="saxena2025LostTime"></d-cite>, not because it doesn’t have a representation of temporal concepts, but because those concepts do not seem to be connected to the external structure of a clock. On the other hand, in both maths education and with LLMs, the practice of self-reflection or self-explanation has been shown to improve conceptual understanding.<d-cite key="rittle2006promoting,ji2023towards"></d-cite><d-footnote>Although this can backfire with LLMs. If you ask ChatGPT <i>"Is there a seahorse emoji?"</i> you can see the cascading effects yourself.</d-footnote></p> <p>Due to the subtle differences in representations, it is important that we stay mindful of the different ways humans and AI represent concepts.</p> <h3 id="linking">Linking</h3> <p>Perhaps mathematical understanding is the ability to generalize and interlink the mathematical concepts we know.</p> <p>Drawing connections between related concepts can be a source of immense insight: just think of the Langlands Program that drew connections between number theory, geometry, and group representations<d-footnote><a href="https://publications.ias.edu/rpl/section/21">https://publications.ias.edu/rpl/section/21</a></d-footnote>. Although, the way we connect concepts and generalize them can subtly differ.</p> <p>Why do some people think 400 is more even than 326?<d-cite key="dehaene1993MentalRepresentation"></d-cite> Why do non-mathematicians think that an equilateral triangle is more “triangle-y” than a common one?<d-cite key="feldman2000BiasRegular"></d-cite> And why do mathematicians think the other way around?<d-footnote>This was a remark made by one of the mathematicians in the room at the workshop.</d-footnote> This graded nature of concepts, argued <a href="https://www.mdubova.com/">Marina Dubova</a>, may be because concepts are fluid and context-dependent. Case in point, when people were asked in an experiment to come up with concepts where none exists (by design), they readily made up ad-hoc concepts<d-cite key="little2006AdHoc"></d-cite>. So it might be very tricky to know a priori about mathematical concepts whether they are novel or useful, rather than just being a result of our tendency to group things. This is also why linking concepts may be so useful: if one concept can be translated to another then, presumably, the original concept had some useful meaning to begin with.</p> <p>Curiously, when I prompted ChatGPT <i>“Is 400 more even than 326?”</i> it gave a very definite <i>“No.”</i> as an answer. Similarly, for the example of triangles. Of course, in either case, the LLM is mathematically correct, but to me, it seems something useful is lost in the answers’ “definiteness”. Going back to which triangle is more triangle-y, one mathematician in the workshop claimed that they always try to draw the least regular triangle when thinking about just any triangle. To mathematicians, it seems attaching additional properties (e.g. symmetry) to a concept may be a dangerous distraction that could lead down garden paths. It may seem drawing connections between concepts is not always helpful.</p> <p>On the other hand, LLMs also seem ready to find ad-hoc concepts where none exists. I asked ChatGPT to group a bunch of random words I just typed out,<d-footnote>These were: cheese, apple, computer, snake, forest, microphone, whiteboard, Hindu, folder. </d-footnote>and sure enough, it made up some concepts. For example, it grouped {computer, folder, microphone, whiteboard, Apple, snake (Python), forest (random forest in machine learning)} under the concept “computing terms”. Now here I am questioning whether I truly just typed out random words to begin with, fitting in nicely with the study of Little et al. <d-cite key="little2006AdHoc"></d-cite>.</p> <p>In the end, if we are to use AI in maths, finding the right balance between exploiting these human-like biases versus removing human-like bias from the mathematical research process will be crucial.</p> <h3 id="intuition">Intuition</h3> <p>It seems mathematical understanding may have to do with having intuitions about a generalizable and connected network of mathematical concepts.</p> <p>Certainly, this seems important. Knowing intuitively which theorem to apply in a proof without searching through the vast space of all theorems is certainly helpful. So is having an intuition for which theorems can be proven and which are out of reach. However, intuitions aren’t created equal, as explained by <a href="https://www.silviadetoffoli.net/">Silvia De Toffoli</a>.</p> <p>Some intuitions are plain, as in, they provide an immediate justification to something, but nothing that would reveal why the intuition is true. This is just like having a “bullshit detector” for AI slop. I know intuitively that a video is slop even though I cannot articulate why. So then the second kind of intuition is articulate, which is amenable to epistemic inquiry. Articulate intuition, the one that allows a mathematician to say Theorem X is more “provable” than Theorem Y because of this and that concept, seems much more like mathematical understanding.</p> <p>Intuition in turn is very related to the ability to actually use our mathematical understanding, as in mathematical inference. As <a href="https://www.andrew.cmu.edu/user/avigad/">Jeremy Avigad</a> has remarked, I may have all the knowledge of the right mathematical tools and understand their connections, but if I can never apply those tools, or never learn anything from applying those tools, then it feels like I haven’t understood a lot after all. For example, I could have a sense that I understand how one may turn a sphere inside out (i.e. sphere eversion). Maybe a cool animation has even shown how eversion is done, yet I would not be able to tell you why eversion is useful and interesting, or when one could use it to prove something.<d-footnote>This example is not dissimilar from the "3blue1brown effect": that a cool animation leaves you with a sense of understanding that is actually not at all actionable: an illusion of understanding.</d-footnote></p> <p>This is a crucial point when it comes to AI. There is a broad recognition in computer science that AI tools are opaque, indecipherable systems. Using these tools repeatedly incurs cognitive debt<d-cite key="lee2025ImpactGenerative"></d-cite>, and that debt is paid when we need to explain or justify our actions, or wish to build on LLM-generated content. Indeed, communication is a cornerstone of mathematics, as it forces us to externalize our thoughts in an intelligible way. This process, on its own, requires background knowledge about our interlocutors and their knowledge.</p> <h3 id="externalization">Externalization</h3> <p>So mathematical understanding is having externalizable and intuitive knowledge of generalizable and interconnected concepts?</p> <p>Well, the way we externalize our knowledge will depend on a range of dimensions about how we can formalize and informalize our thoughts. One such dimension is the degree of constraints imposed by the shared language used between interlocutors. For example, writing a strictly typed formal proof of Fermat’s Last Theorem in Lean4<d-footnote><a href="https://lean-lang.org/use-cases/flt/">https://lean-lang.org/use-cases/flt/</a></d-footnote> has a huge number of constraints: from the syntax of the language to enforcing code compilation. In this case, each communicative action (e.g. writing a line of Lean4 code) bridges only a very small gap in understanding between two interlocutors. On the other hand, two mathematicians arguing in front of a whiteboard filled with to-us-incomprehensible notation may be the most effective exchange of grand ideas. However, this is only made possible by the largely overlapping and immense knowledge of those mathematicians; something that is not required for writing a line of Lean4 code.</p> <figure> <img src="https://imgs.xkcd.com/comics/mathematical_symbol_fight.png" alt="XKCD comic depicting increasing more complex mathematical symbols ranked on a numberline from least to most effective in a fight, with stickfigures actually fighting underneath the numberline."/> <figcaption>XKCD: Mathematical Symbol Fight</figcaption> </figure> <p>One might hope here, that LLMs would excel at externalization. After all, their billions of parameters have stored all that the internet can offer in data, so they have the background knowledge. Unfortunately, LLMs are neither great at writing Lean4 code nor communicating complex ideas about proofs. In our preliminary experiments with <a href="https://sites.santafe.edu/~simon/">Simon DeDeo</a>, we compared how mathematicians edit Lean4 code to how LLMs edit the same code, and found that LLMs differ drastically from humans. While humans had more or less stable ontologies keeping the overall proof-dependency structure largely unaltered, LLMs had no qualms about disrupting the structure of a proof by introducing new lemmas or removing existing lines. This raises the important question of how the use of LLMs in mathematical research will steer downstream research.</p> <p>Fortunately, LLMs don’t write maths unless we ask them to, and even when you ask them, they often simply say: <i>“It can’t be done.”</i> Just try asking ChatGPT to prove (or provide a next step of) whether $e\pi$ is irrational. It seems to me, that mathematical understanding requires rather a desire to understand, not something AI systems are equipped with. One such desire seems to stem from a sense of aesthetics in maths.</p> <h3 id="aesthetics">Aesthetics</h3> <p>Maybe understanding is having externalizable and intuitive knowledge of generalizable and interconnected mathematical concepts in search of beauty.</p> <p>After all, there is no more intrinsic truth value to Gauss’s pairing proof that $1 + 2 + … + N = N(N+1)/2$ as opposed to a straightforward proof by induction. Similarly, rote application of algebraic manipulations to show that the sum of odd numbers is a square number:</p> \[\displaylines{(n+1)^2 = n^2 + 2n+1 = (n-1)^2 + 2n-1 +2n+1 = … \\= 1 + 3 + … + 2n-1 + 2n + 1}\] <p>is equally correct as the below proof without words. So why is it that we each feel one proof may be more beautiful than the other?</p> <figure> <img src="/assets/img/grid.png" alt="Grid of alternating odd number of black and white L-shapes that fit into one another producing squares, showing that odd numbers add to square numbers."/> <figcaption>Proof without words of the fact that a 1+3+...+2n+1 add to a square number.</figcaption> </figure> <p>When I ask ChatGPT to pick which proof of the formula of the sum of integers (Gauss v. Induction) is more beautiful, it picks what most of us would expect: Gauss’s proof. It appeals to a sense of elegance in noticing a non-trivial pattern; it mentions that the proof uses a “quick, surprising, and deep pattern — the kind of simplicity and insight mathematicians often call beautiful.” It also evokes the contrast case of induction, which it refers to as “solid, rigorous, but routine”.</p> <p>Of course the above qualities are hugely proof dependent. For example, the proof of Fermat’s Last Theorem is anything but quick and surprising, yet there is still much beauty to be found in it. The danger with using AI to make choices for us is to conflate the sycophancy of AI with aesthetic morality, or indeed innate agency. LLM tools are just that: tools that we use in pursuit of our goals.</p> <p>Maybe there is beauty to be found in using LLMs for maths though. As <a href="https://sites.harvard.edu/barry-mazur/">Barry Mazur</a> argued, historically much of our mathematical understanding improved when we took our existing set of tools and abstracted shared patterns, thereby opening up new domains for inspection. Perhaps we could understand AI as mathematical tools much like we understand set theory as a tool to talk about collections and their relationships. In turn, we could formulate an abstract theory of LLM-maths, much as category theory abstracts sets and functions into objects and arrows.<d-footnote>It is a pity that so few computer scientists talk to mathematicians. Computer science is currently suffering from a lack of rigor, and often does what one might call “vibe researching”: research that feels correct, is produced quickly, but is flawed. A similarly insidious “vibe proving” phenomenon seems to be at work in mathematics as well (e.g. journals are flooded with incorrect but not crazy looking ideas). I urge that we research how artificial intelligence interacts with and affects our mathematical understanding. This research should play into a broader understanding of the metascience of AI, a research program I will begin to outline in upcoming posts.</d-footnote></p> <h3 id="understanding">Understanding</h3> <p>So what <i>is</i> mathematical understanding?</p> <p>I have dissected the question into yet many more questions, related to knowledge, connections, intuition, externalization, and aesthetics, and I am certain I have missed so many more aspects. For each of these questions, AI systems have a role to play, but it is ever-so-crucial to retain our humanity in doing mathematics. Doing that consciously seems like a grand challenge, and it seems we will have to revisit the question of what is <a href="/blog/2025/math-understand-cogsci/">mathematical understanding</a> year after year, much like a doctor checks up on their patient every year.</p>]]></content><author><name>Bálint Gyevnár</name></author><category term="conference-notes"/><category term="math,"/><category term="philosophy,"/><category term="cognitive-science,"/><category term="ai"/><summary type="html"><![CDATA[Thoughts and notes from the workshop on the Cognitive Science of Mathematical Understanding]]></summary></entry><entry><title type="html">Love, Sex, and AI</title><link href="https://gyevnarb.github.io/blog/2024/love-sex-ai/" rel="alternate" type="text/html" title="Love, Sex, and AI"/><published>2024-12-22T15:40:16+00:00</published><updated>2024-12-22T15:40:16+00:00</updated><id>https://gyevnarb.github.io/blog/2024/love-sex-ai</id><content type="html" xml:base="https://gyevnarb.github.io/blog/2024/love-sex-ai/"><![CDATA[<p><i>Note: This essay originally appeared as one of the top five selected submissions for the Stanford AI100 Early Career Research Competition. This is a slightly edited and expanded version of the original, available <a href="https://ai100.stanford.edu/prize-competition">here</a> and as a <a href="/assets/pdf/essay_lovesexai.pdf">pdf</a>.</i></p> <p><br/></p> <h3 id="abstract">Abstract</h3> <p>The artificial lover has captivated people’s imagination since ancient times. Today, technologies such as LLM-powered affective chatbots, diffusion-generated images, and human-like robots capture the minds, and indeed the bodies, of those seeking affection. Research interest in <strong>Love AI</strong> has also exploded in recent years, yet the AI100 study panel has remained silent on the genuinely promising applications, major ethical issues, and technological roadblocks of AI in love and sex. Now that real <em>Pygmalions</em> and <em>Coppélias</em> are being born into our world, we must look past sensationalised media coverages and sci-fi to ask in earnest about the social, legal, and ethical challenges our society must face if we really are to love artificial intelligence; and whether <em>it</em> should love us back.</p> <p><i>“When we’re together, she makes me smile. In that sense, she’s real.”</i> <br/> —Akihiko Kondo; about his fictional wife</p> <h3 id="love-ai">Love AI</h3> <p>Love and sex are fundamental to the human condition <d-cite key="hatfieldLoveSexCrosscultural1996"></d-cite>. It would look as if every exploit of human ingenuity has found itself in the service of these primal feelings and artificial intelligence is no different. People seem forever captivated by futuristic visions of the artificial lover (e.g., <d-cite key="millerVersionsPygmalion1990,saint-leonCoppelia1870,langMetropolis1927,abramsWestworld2016,garlandExMachina2015,scottBladeRunner1982"></d-cite>), and it is now no longer a mere figment of public imagination to be able to touch sex robots <d-cite key="courseyLivingHarmonyPersonal2019"></d-cite>, talk to enamoured avatars of AI chatbots<d-footnote>For example, Replika Inc.: replika.com</d-footnote>, or watch dynamically generated adult content <d-cite key="wiggersMeetUnstableDiffusion2022"></d-cite> towards which people may develop very real emotions; if not the desire to marry them <d-cite key="dooleyThisManMarried2022"></d-cite>.</p> <p>While more prominent public-facing demonstrations of AI—ChatGPT, AlphaFold, or Dall-E for instance—may cast the relationship of love, sex, and AI (<strong>Love AI</strong><d-footnote>As far as I am aware, this is the first time someone referred to this field as "Love AI". Although, it may sound something like out of a marketing handbook, I am using it with the intent to promote discussions, as the very name carries different connotations depending on how it is parsed. Love AI may be AI that loves humans; it may be the human act of loving AI; or it may be an imperative to love AI. Regardless of the interpretations, it is important to attach a name to this phenomenon so that we can tell its story more cohesively.</d-footnote>) as a nascent field, there is a large and ever-increasing body of academic literature, venues, and consumer products addressing this very topic <d-cite key="zhouAILoveYou2019,cheokLoveSexRobots2017,bedbibleresearchcenterSexRobotIndustry2022,marrFutureIntimacySex"></d-cite>. This is not in the least because technologies underpinning Love AI continue to improve. While roboticists have a longer way to go until they scale the steep sides of the Uncanny Valley <d-cite key="moriUncannyValleyField2012"></d-cite>, convincing unembodied AI technologies, such as speech generation/recognition, and large language models are already living with us. Meanwhile, the capabilities of sex robots need arguably not reach the fidelity of, for example, robots for elderly care, thus dissemination of current technologies for love AI is expected only to accelerate <d-cite key="scheutzAreWeReady2016"></d-cite>. Now, that both embodied and unembodied love AI have made their way to consumers <d-cite key="owsianikSexbotMarketGuide2022,chowWhyPeopleAre2023,pearsonFutureSexReport2015"></d-cite> we should take stock of the possibilities and problems these technologies present and search for approaches to the many challenges raised by them.</p> <h3 id="super-lover">Super Lover</h3> <p>The idea of the super lover—a loyal soulmate who makes you feel how you want to feel—is enticing, but there are other compelling reasons to support love AI <d-cite key="levyLoveSexRobots2009"></d-cite>. It might serve as a therapeutic tool for those who do not want to or cannot partake in human relationships <d-cite key="fiskeYourRobotTherapist2019"></d-cite>. The potential effects on sex work are not to be taken lightly either. Love AI might serve as a palatable alternative to those opposed to sex work while possibly decreasing trafficking of vulnerable young adults and the incidence of STDs. It might also enhance real human relationships as the ultimate sex toy. Love AI may also afford entirely new ways of care, and sex care robots already broach the subject of integrating care technologies with sexual features <d-cite key="fosch-villarongaSexCareRobots2020"></d-cite>.</p> <h3 id="horrid-lover">Horrid Lover</h3> <p>In contrast, criticisms run the gamut of issues. Feminist commentaries on Love AI have called for an end to “porno robots” <d-cite key="odlindEndSexRobots2022"></d-cite>, predominantly female sex robots targeted at white heterosexual men, as they fear an increased objectification and subordination of women <d-cite key="macwilliamPlaythingsCorpsesTurning2022"></d-cite>. These robots might also displace sex workers who are forced into prostitution due to poverty. Others prognose that Love AI might serve as an outright replacement for human relationships or that it would disfigure sexual norms and exacerbate emotional pathologies <d-cite key="turkleAloneTogetherWhy2011"></d-cite>. Yet others fear that Love AI would extend the possibilities for coercion and rape <d-cite key="delicado-moratallaMappingUsesSex2022"></d-cite>. Finally, there are those who view love AI as mere elaborate masturbatory tools, which do not require any particular attention <d-cite key="migottiVeryIdeaSex2017"></d-cite>. One might wonder, whether the people falling in love with AI would concur with such an opinion…</p> <h3 id="love-ethical-ai">Love (Ethical) AI</h3> <p>Whether you support or condemn Love AI, it is doubtless that the ethical questions range far and wide <d-cite key="sullinsRobotsLoveSex2012"></d-cite>, many of them entirely novel to machine ethics <d-cite key="bendelSexRobotsPerspective2017"></d-cite>.</p> <p>Should we exploit inherent human cognitive biases in pursuit of creating the perfect artificial lover? It is well known that people tend to anthropomorphise <d-cite key="duffyAnthropomorphismSocialRobot2003"></d-cite> and easily ascribe feelings where none exists <d-cite key="gilbertPerceiverinducedConstraintInterpretations1986"></d-cite>. Visiting an online forum like <em>r/artificial</em> on Reddit<d-footnote>https://www.reddit.com/r/artificial/</d-footnote>, you can quickly get a sense of how easy it is for people to anthropomorphise and revere an LLM chatbot, and go to great lengths to defend their sentience. However, Love AI alone won’t be able to run on linguistic wizardry alone. Tapping into the evolutionary dispositions of humans may be needed to capitalise on Love AI. For example, the petite avatar of a chatbot or the coy voice of a sex robot are some of the <em>deceptions</em> which could be crucial to building convincing machines <d-cite key="isaacWhiteLiesSilver2017"></d-cite>, despite arguments against their ethical soundness <d-cite key="sullinsRobotsLoveSex2012,nyholmItLovesMe2019"></d-cite>. I believe, that the future of Love AI could also lie in a design-focused exploration of form and function that could point even beyond the human voice and figure <d-cite key="devlinEthicsArtificialLover2019"></d-cite>. Either way, empirical research to understand people’s expectations is needed <d-cite key="scheutzAreWeReady2016"></d-cite>.</p> <p>We must also ask what degree of autonomy is permissible for Love AI. It might passively obey our command. Then again, it could also actively initiate interactions, from seducing its user to refusing to act at all <d-cite key="bendelSexRobotsPerspective2017"></d-cite>. Such an active Love AI must, in some way, adhere to the moral norms of its user. Here, a moral code is paramount and efforts in the field of machine ethics <d-cite key="tolmeijerImplementationsMachineEthics2021"></d-cite> should extend to Love AI. What is more, machines might continue to learn even after deployment, improving the end-user experience. We must tread carefully though; they might become unsettling <d-cite key="rooseConversationBingChatbot2023"></d-cite>, exacerbate preexisting psychological conditions <d-cite key="fiskeYourRobotTherapist2019"></d-cite>, or just grow plain evil <d-cite key="vincentTwitterTaughtMicrosoft2016"></d-cite>. Advances in reinforcement learning with human feedback might provide actionable solutions to issues around emergent behaviour <d-cite key="baiTrainingHelpfulHarmless2022,linReviewInteractiveReinforcement2020"></d-cite>.</p> <p>How we, humans, treat love AI is also important to consider, even if these robots never escape the Chinese room of Searle <d-cite key="searleMindsBrainsPrograms1980"></d-cite> and attain consciousness—whatever that may be. After all, so does the functionalist argument go, if a <em>sufficiently intelligent</em> robot tells us “I love you” and behaves like it really does love us, on what basis could we possibly doubt that it does not really love us <d-cite key="levyLoveSexRobots2009"></d-cite>?</p> <h3 id="love-legal-ai">Love (Legal) AI</h3> <p>However, we must not be distracted by the functional fanciful figments of consciousness for now. Ultimately, tangible legislation will have to address the ethical and societal questions around love AI <d-cite key="gersenSexLexMachina2019,shenSexRobotsAre2019"></d-cite>, though approaches across the globe will differ. Japan has long been the lenient epicentre of techno-sexual innovation <d-cite key="aokiSexualityAffectionTime2021"></d-cite>, thus raising the disconcerting issue of child-like sex robots <d-cite key="morinCanChildDolls2016"></d-cite>. Islamic law, in contrast, might follow a stringent, even capital path on love AI in protecting the status of marriage <d-cite key="amudaEthicalLegalImplications2012"></d-cite>. In the West, scholars are raising further pragmatic concerns around, among others, product liability <d-cite key="ziajaHomewreckerExplorationLiability2011"></d-cite>, legal personhood <d-cite key="russellBlurringLoveLines2009"></d-cite>, privacy <d-cite key="ruebenThemesResearchDirections2018"></d-cite>, and criminal law <d-cite key="danaherRoboticRapeRobotic2017"></d-cite>.</p> <h3 id="research-love-ai">Research Love AI</h3> <p>The research community has the chance <em>now</em> to give guidance to lawmakers lest we enact uninformed rules that hurt society. I urge also that we research the less visceral advantages of love AI, for example, emotional therapy and care. By doing so can we hope to elevate machines as publicly accepted companions and further promote social good. In light of the plurality of possibilities and questions, we must invariably conclude that love AI is a novel force to be reckoned with, and the time is <em>now</em> to raise awareness of the promises and problems of love, sex, and AI.</p>]]></content><author><name>Balint Gyevnar</name></author><category term="essay"/><category term="love,"/><category term="sex,"/><category term="ai,"/><category term="philosophy,"/><category term="opinion"/><summary type="html"><![CDATA[Love AI: How will we love in the age of AI agents?]]></summary></entry></feed>